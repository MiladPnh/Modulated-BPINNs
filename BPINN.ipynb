{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "fcec9141-c387-4649-a26a-318f75e8db16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Master DataFrame successfully created ---\n",
      "   t_min         C  experiment_id  porosity  inflow_rate_Q     C0       v_x  \\\n",
      "0    0.0  0.000000            1.0     0.371            0.4  100.0  5.362363   \n",
      "1   13.0  0.000000            1.0     0.371            0.4  100.0  5.362363   \n",
      "2   27.0  0.000000            1.0     0.371            0.4  100.0  5.362363   \n",
      "3   40.0  0.000000            1.0     0.371            0.4  100.0  5.362363   \n",
      "4   54.0  0.872917            1.0     0.371            0.4  100.0  5.362363   \n",
      "\n",
      "   C_over_C0  \n",
      "0   0.000000  \n",
      "1   0.000000  \n",
      "2   0.000000  \n",
      "3   0.000000  \n",
      "4   0.008729  \n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 329 entries, 0 to 328\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   t_min          329 non-null    float32\n",
      " 1   C              329 non-null    float32\n",
      " 2   experiment_id  329 non-null    float32\n",
      " 3   porosity       329 non-null    float32\n",
      " 4   inflow_rate_Q  329 non-null    float32\n",
      " 5   C0             329 non-null    float32\n",
      " 6   v_x            329 non-null    float32\n",
      " 7   C_over_C0      329 non-null    float32\n",
      "dtypes: float32(8)\n",
      "memory usage: 10.4 KB\n",
      "\n",
      "--- Normalization Constants Defined ---\n",
      "Time will be scaled by: 436.00\n",
      "Position will be scaled by: 92.00\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SIMPLIFIED & ROBUST DATA LOADING\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- 1. Helper function to create dummy CSV files for verification ---\n",
    "def create_dummy_csv_files():\n",
    "    \"\"\"Creates two dummy CSV files to simulate the experimental data.\"\"\"\n",
    "    print(\"Creating dummy CSV files for verification...\")\n",
    "\n",
    "    # Data for Experiment 1\n",
    "    data1 = {\n",
    "        'time_min': [0, 13, 54, 120, 224, 436],\n",
    "        'C_out': [0.0, 0.0, 0.87, 8.34, 61.8, 98.48]\n",
    "    }\n",
    "    df1 = pd.DataFrame(data1)\n",
    "    df1.to_csv(\"exp_1.csv\", index=False)\n",
    "\n",
    "    # Data for Experiment 2\n",
    "    data2 = {\n",
    "        'time_min': [0, 10, 40, 80, 150, 200],\n",
    "        'C_out': [0.0, 1.0, 15.0, 65.0, 98.0, 99.0]\n",
    "    }\n",
    "    df2 = pd.DataFrame(data2)\n",
    "    df2.to_csv(\"exp_2.csv\", index=False)\n",
    "    print(\"Dummy files 'exp_1.csv' and 'exp_2.csv' created.\")\n",
    "\n",
    "\n",
    "# --- 2. Main function to load all data into a single DataFrame ---\n",
    "def create_master_dataframe(file_paths, experiment_params_list):\n",
    "    \"\"\"\n",
    "    Reads multiple CSV files and combines them with their experimental\n",
    "    parameters into a single, master Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        file_paths (list of str): A list of paths to the CSV data files.\n",
    "        experiment_params_list (list of dict): A list of dictionaries, where each\n",
    "                                               dictionary contains the parameters\n",
    "                                               for the corresponding file.\n",
    "    Returns:\n",
    "        pandas.DataFrame: A single DataFrame containing all experimental data.\n",
    "    \"\"\"\n",
    "    all_experiments_df_list = []\n",
    "\n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        # --- A. Read the time-series data from the CSV ---\n",
    "        temp_df = pd.read_csv(file_path)\n",
    "        # --> IMPORTANT: Adjust these column names if yours are different\n",
    "        temp_df.rename(columns={'time_min': 't_min', 'C_out': 'C'}, inplace=True)\n",
    "\n",
    "        # --- B. Get the corresponding parameters for this file ---\n",
    "        params = experiment_params_list[i]\n",
    "\n",
    "        # --- C. Add parameter columns to the DataFrame ---\n",
    "        temp_df['experiment_id'] = i + 1 # Unique ID for each experiment\n",
    "        temp_df['porosity'] = params['porosity_middle_layer']\n",
    "        temp_df['inflow_rate_Q'] = params['inflow_rate_Q']\n",
    "        temp_df['C0'] = params['C0']\n",
    "        \n",
    "        all_experiments_df_list.append(temp_df)\n",
    "\n",
    "    # --- D. Concatenate all individual DataFrames into one master DataFrame ---\n",
    "    master_df = pd.concat(all_experiments_df_list, ignore_index=True).astype('float32')\n",
    "\n",
    "    return master_df\n",
    "\n",
    "# --- 3. Define the physical constants and the experimental parameters ---\n",
    "\n",
    "# Physical constants\n",
    "DIAMETER = 16.0\n",
    "RADIUS = DIAMETER / 2.0\n",
    "AREA_MM2 = np.pi * RADIUS**2\n",
    "\n",
    "# ==============================================================================\n",
    "# >> USER ACTION REQUIRED HERE <<\n",
    "# Manually define the parameters for each of your 9 CSV files below.\n",
    "# I have created 2 dummy examples. You should replace this with your 9 files.\n",
    "# ==============================================================================\n",
    "\n",
    "# List of your CSV file paths\n",
    "csv_files = [\"exp_1.csv\", \"exp_2.csv\", \"exp_3.csv\", \"exp_4.csv\", \"exp_5.csv\", \"exp_6.csv\", \"exp_7.csv\", \"exp_8.csv\", \"exp_9.csv\"] # <-- REPLACE WITH YOUR 9 FILE PATHS\n",
    "\n",
    "# List of corresponding parameters for each file\n",
    "# Each dictionary represents one experiment.\n",
    "params_list = [\n",
    "    # Parameters for exp_1.csv\n",
    "    {\n",
    "        \"inflow_rate_Q\": 0.4,   # ml/min\n",
    "        \"C0\": 100.0,            # mg/L\n",
    "        \"porosity_middle_layer\": 0.371\n",
    "    },\n",
    "    # Parameters for exp_2.csv\n",
    "    {\n",
    "        \"inflow_rate_Q\": 1.0,   # ml/min\n",
    "        \"C0\": 100.0,            # mg/L\n",
    "        \"porosity_middle_layer\": 0.371\n",
    "    },\n",
    "    # Parameters for exp_3.csv\n",
    "    {\n",
    "        \"inflow_rate_Q\": 2.5,   # ml/min\n",
    "        \"C0\": 100.0,            # mg/L\n",
    "        \"porosity_middle_layer\": 0.371\n",
    "    },\n",
    "    # Parameters for exp_4.csv\n",
    "    {\n",
    "        \"inflow_rate_Q\": 0.4,   # ml/min\n",
    "        \"C0\": 150.0,            # mg/L\n",
    "        \"porosity_middle_layer\": 0.371\n",
    "    },\n",
    "    # Parameters for exp_5.csv\n",
    "    {\n",
    "        \"inflow_rate_Q\": 0.4,   # ml/min\n",
    "        \"C0\": 200.0,            # mg/L\n",
    "        \"porosity_middle_layer\": 0.371\n",
    "    },\n",
    "    # Parameters for exp_6.csv\n",
    "    {\n",
    "        \"inflow_rate_Q\": 0.4,   # ml/min\n",
    "        \"C0\": 250.0,            # mg/L\n",
    "        \"porosity_middle_layer\": 0.371\n",
    "    },\n",
    "    # Parameters for exp_7.csv\n",
    "    {\n",
    "        \"inflow_rate_Q\": 0.4,   # ml/min\n",
    "        \"C0\": 300.0,            # mg/L\n",
    "        \"porosity_middle_layer\": 0.371\n",
    "    },\n",
    "    # Parameters for exp_8.csv\n",
    "    {\n",
    "        \"inflow_rate_Q\": 0.4,   # ml/min\n",
    "        \"C0\": 100.0,            # mg/L\n",
    "        \"porosity_middle_layer\": 0.424\n",
    "    },\n",
    "    # Parameters for exp_9.csv\n",
    "    {\n",
    "        \"inflow_rate_Q\": 0.4,   # ml/min\n",
    "        \"C0\": 100.0,            # mg/L\n",
    "        \"porosity_middle_layer\": 0.399\n",
    "    }\n",
    "]\n",
    "\n",
    "# --- 4. Main execution block ---\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Load all data into the master DataFrame\n",
    "    master_df = create_master_dataframe(csv_files, params_list)\n",
    "    \n",
    "    # --- 5. Calculate derived values needed for the model ---\n",
    "    # Calculate pore water velocity 'v_x' (mm/min) for each experiment\n",
    "    # Note: 1 ml = 1000 mm^3\n",
    "    master_df['v_x'] = (master_df['inflow_rate_Q'] * 1000.0) / (AREA_MM2 * master_df['porosity'])\n",
    "\n",
    "    # Calculate the dimensionless concentration 'C/C0'\n",
    "    master_df['C_over_C0'] = master_df['C'] / master_df['C0']\n",
    "    \n",
    "    # Display the final, processed DataFrame\n",
    "    print(\"\\n--- Master DataFrame successfully created ---\")\n",
    "    print(master_df.head())\n",
    "    print(\"\\nDataFrame Info:\")\n",
    "    master_df.info()\n",
    "\n",
    "    P_MIN = torch.tensor(master_df[['porosity', 'v_x', 'C0']].min().values, dtype=torch.float32, device=device)\n",
    "    P_RANGE = torch.tensor(master_df[['porosity', 'v_x', 'C0']].max().values - master_df[['porosity', 'v_x', 'C0']].min().values, dtype=torch.float32, device=device)\n",
    "    T_MAX = torch.tensor(master_df['t_min'].max(), dtype=torch.float32, device=device)\n",
    "    L_MAX = torch.tensor(TOTAL_LENGTH, dtype=torch.float32, device=device)\n",
    "\n",
    "    print(\"\\n--- Normalization Constants Defined ---\")\n",
    "    print(f\"Time will be scaled by: {T_MAX.item():.2f}\")\n",
    "    print(f\"Position will be scaled by: {L_MAX.item():.2f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "aef326d6-3633-431a-8ca6-fffeb378ad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.to_csv('master_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c54146-4a9d-4638-8a6e-cb58d24a5139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "abf723ac-982f-4836-a9bf-8fe55c0c33c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "--- Testing Full FiLM-based BNN Architecture ---\n",
      "\n",
      "Modulator 'z' output shape: torch.Size([32, 64])\n",
      "Solver 'mu' output shape: torch.Size([32, 1])\n",
      "Total KL loss aggregated: 5127.1465\n",
      "\n",
      "SUCCESS: The full PyTorch architecture is defined and verified.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PHASE 1, STEP 2: FULL FILM-BASED BNN ARCHITECTURE (PYTORCH)\n",
    "# ==============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Constants (from your TF code) ---\n",
    "NUM_KNOWN_PARAMS = 3\n",
    "LATENT_DIM = 64\n",
    "COORD_DIM = 2\n",
    "OUTPUT_DIM = 1\n",
    "LAYER_WIDTH = 50\n",
    "NUM_HIDDEN_LAYERS = 2\n",
    "\n",
    "# --- Custom Bayesian Layer (Verified from last step) ---\n",
    "class DenseBayesian(nn.Module):\n",
    "    def __init__(self, in_features, out_features, activation=None):\n",
    "        super().__init__()\n",
    "        self.in_features, self.out_features = in_features, out_features\n",
    "        self.kernel_mu = nn.Parameter(torch.empty(in_features, out_features))\n",
    "        self.kernel_rho = nn.Parameter(torch.empty(in_features, out_features))\n",
    "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "        self.bias_rho = nn.Parameter(torch.empty(out_features))\n",
    "        nn.init.xavier_uniform_(self.kernel_mu)\n",
    "        nn.init.normal_(self.kernel_rho, mean=-2., std=.1)\n",
    "        nn.init.zeros_(self.bias_mu)\n",
    "        nn.init.normal_(self.bias_rho, mean=--2., std=.1)\n",
    "        self.prior = torch.distributions.Laplace(0, .1)\n",
    "        self.activation = activation() if activation else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        kernel_sigma = torch.nn.functional.softplus(self.kernel_rho)\n",
    "        bias_sigma = torch.nn.functional.softplus(self.bias_rho)\n",
    "        kernel_posterior = torch.distributions.Normal(self.kernel_mu, kernel_sigma)\n",
    "        bias_posterior = torch.distributions.Normal(self.bias_mu, bias_sigma)\n",
    "        w, b = kernel_posterior.rsample(), bias_posterior.rsample()\n",
    "        kl_divergence = (kernel_posterior.log_prob(w) - self.prior.log_prob(w)).sum() + \\\n",
    "                        (bias_posterior.log_prob(b) - self.prior.log_prob(b)).sum()\n",
    "        return self.activation(torch.matmul(x, w) + b), kl_divergence\n",
    "\n",
    "# --- 1. Modulator and FiLM Layer Definitions (PyTorch) ---\n",
    "class ModulatorNetwork(nn.Module):\n",
    "    \"\"\"Encodes known experimental parameters [ε, v_x, C₀] into a latent vector 'z'.\"\"\"\n",
    "    def __init__(self, input_param_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        # self.net = nn.Sequential(\n",
    "        #     nn.Linear(input_param_dim, 128),\n",
    "        #     nn.Tanh(),\n",
    "        #     nn.Linear(128, 128),\n",
    "        #     nn.Tanh(),\n",
    "        #     nn.Linear(128, latent_dim)\n",
    "        # )\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_param_dim, latent_dim)\n",
    "        )\n",
    "    def forward(self, physical_params):\n",
    "        return self.net(physical_params)\n",
    "\n",
    "class FiLMLayer(nn.Module):\n",
    "    \"\"\"Feature-wise Linear Modulation Layer.\"\"\"\n",
    "    def __init__(self, num_features, latent_dim):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.projection = nn.Linear(latent_dim, num_features * 2)\n",
    "\n",
    "    def forward(self,x,z):\n",
    "        gamma,beta=self.projection(z).chunk(2,dim=-1);\n",
    "        return gamma*x+beta\n",
    "\n",
    "\n",
    "# --- 2. The Main Bayesian Solver Model ---\n",
    "class BayesianSolverWithFiLM(nn.Module):\n",
    "    \"\"\"A Bayesian Solver that uses FiLM layers for conditioning.\"\"\"\n",
    "    def __init__(self, coord_dim, output_dim, layer_width, latent_dim, num_hidden_layers):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.input_dense = DenseBayesian(coord_dim, layer_width, activation=nn.Tanh)\n",
    "        \n",
    "        self.hidden_blocks = nn.ModuleList()\n",
    "        for _ in range(num_hidden_layers):\n",
    "            block = nn.ModuleDict({\n",
    "                'dense': DenseBayesian(layer_width, layer_width),\n",
    "                'film': FiLMLayer(layer_width, latent_dim),\n",
    "                'activation': nn.Tanh()\n",
    "            })\n",
    "            self.hidden_blocks.append(block)\n",
    "\n",
    "        self.output_dense = DenseBayesian(layer_width, output_dim * 2)\n",
    "\n",
    "    def forward(self, coords, z):\n",
    "        total_kl_loss = 0.\n",
    "        \n",
    "        x, kl = self.input_dense(coords)\n",
    "        total_kl_loss += kl\n",
    "        \n",
    "        for block in self.hidden_blocks:\n",
    "            x_res, kl = block['dense'](x)\n",
    "            total_kl_loss += kl\n",
    "            \n",
    "            x_mod = block['film'](x_res, z)\n",
    "            # Add a residual connection (as in your TF code)\n",
    "            x = block['activation'](x_mod) + x_res\n",
    "\n",
    "        raw_output, kl = self.output_dense(x)\n",
    "        total_kl_loss += kl\n",
    "        \n",
    "        mu = raw_output[..., :self.output_dim]\n",
    "        sigma = torch.nn.functional.softplus(raw_output[..., self.output_dim:]) + 1e-6\n",
    "        \n",
    "        return torch.distributions.Normal(loc=mu, scale=sigma), total_kl_loss\n",
    "\n",
    "# --- 3. Sanity Check Cell ---\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\n--- Testing Full FiLM-based BNN Architecture ---\")\n",
    "\n",
    "    # 1. Instantiate models\n",
    "    modulator = ModulatorNetwork(NUM_KNOWN_PARAMS, LATENT_DIM).to(device)\n",
    "    solver = BayesianSolverWithFiLM(COORD_DIM, OUTPUT_DIM, LAYER_WIDTH, LATENT_DIM, NUM_HIDDEN_LAYERS).to(device)\n",
    "\n",
    "    # 2. Create dummy data\n",
    "    batch_size = 32\n",
    "    dummy_params = torch.randn(batch_size, NUM_KNOWN_PARAMS).to(device)\n",
    "    dummy_coords = torch.randn(batch_size, COORD_DIM).to(device)\n",
    "\n",
    "    # 3. Perform a forward pass\n",
    "    z_out = modulator(dummy_params)\n",
    "    output_dist, total_kl = solver(dummy_coords, z_out)\n",
    "    mu_out = output_dist.mean\n",
    "\n",
    "    # 4. Verify outputs\n",
    "    print(f\"\\nModulator 'z' output shape: {z_out.shape}\")\n",
    "    assert z_out.shape == (batch_size, LATENT_DIM)\n",
    "\n",
    "    print(f\"Solver 'mu' output shape: {mu_out.shape}\")\n",
    "    assert mu_out.shape == (batch_size, OUTPUT_DIM)\n",
    "    \n",
    "    print(f\"Total KL loss aggregated: {total_kl.item():.4f}\")\n",
    "    assert total_kl.ndim == 0\n",
    "\n",
    "    print(\"\\nSUCCESS: The full PyTorch architecture is defined and verified.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6f5ca034-04af-47f7-906f-ae07ee7b2fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing the PyTorch Dataset and DataLoader ---\n",
      "Dataset created with 100 samples.\n",
      "DataLoader created with batch size 16.\n",
      "\n",
      "--- Inspecting the first batch ---\n",
      "Parameters shape: torch.Size([16, 3])\n",
      "Time shape:       torch.Size([16, 1])\n",
      "Conc shape:       torch.Size([16, 1])\n",
      "\n",
      "SUCCESS: The data handling pipeline is correctly defined and verified.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PHASE 2: DATA HANDLING WITH PYTORCH DATASET AND DATALOADER\n",
    "# ==============================================================================\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. The PyTorch Dataset Class ---\n",
    "class BreakthroughDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset for the breakthrough curve data.\"\"\"\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "\n",
    "        # Extract columns into numpy arrays for efficiency\n",
    "        self.params = self.df[['porosity', 'v_x', 'C0']].values\n",
    "        self.times = self.df[['t_min']].values\n",
    "        self.concentrations = self.df[['C_over_C0']].values\n",
    "\n",
    "    def __len__(self):\n",
    "        # The total number of samples is the number of rows in the DataFrame\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve a single sample by its index\n",
    "        params = self.params[idx]\n",
    "        time = self.times[idx]\n",
    "        concentration = self.concentrations[idx]\n",
    "        \n",
    "        # Return as PyTorch tensors of type float32\n",
    "        return {\n",
    "            'params': torch.tensor(params, dtype=torch.float32),\n",
    "            't': torch.tensor(time, dtype=torch.float32),\n",
    "            'c_over_c0': torch.tensor(concentration, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "# --- 2. Sanity Check Cell ---\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\n--- Testing the PyTorch Dataset and DataLoader ---\")\n",
    "    \n",
    "    # Assume master_df has been created from the previous data loading script\n",
    "    # For this test, we'll create a small dummy DataFrame\n",
    "    dummy_data = {\n",
    "        'porosity': np.random.rand(100),\n",
    "        'v_x': np.random.rand(100) * 5 + 5,\n",
    "        'C0': np.random.rand(100) * 100 + 100,\n",
    "        't_min': np.random.rand(100) * 500,\n",
    "        'C_over_C0': np.random.rand(100)\n",
    "    }\n",
    "    master_df_dummy = pd.DataFrame(dummy_data).astype(np.float32)\n",
    "\n",
    "    # 1. Instantiate the Dataset\n",
    "    dataset = BreakthroughDataset(master_df_dummy)\n",
    "    print(f\"Dataset created with {len(dataset)} samples.\")\n",
    "    \n",
    "    # 2. Instantiate the DataLoader\n",
    "    batch_size = 16\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    print(f\"DataLoader created with batch size {batch_size}.\")\n",
    "\n",
    "    # 3. Retrieve and inspect one batch\n",
    "    first_batch = next(iter(data_loader))\n",
    "\n",
    "    print(\"\\n--- Inspecting the first batch ---\")\n",
    "    p = first_batch['params']\n",
    "    t = first_batch['t']\n",
    "    c = first_batch['c_over_c0']\n",
    "\n",
    "    print(f\"Parameters shape: {p.shape}\")\n",
    "    print(f\"Time shape:       {t.shape}\")\n",
    "    print(f\"Conc shape:       {c.shape}\")\n",
    "\n",
    "    assert p.shape == (batch_size, 3)\n",
    "    assert t.shape == (batch_size, 1)\n",
    "    assert c.shape == (batch_size, 1)\n",
    "    assert p.dtype == torch.float32\n",
    "\n",
    "    print(\"\\nSUCCESS: The data handling pipeline is correctly defined and verified.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "474c50b7-bc3b-459b-b8a1-837a8b240291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "--- Testing NORMALIZED Physics Loss ---\n",
      "Physics loss computed: 7.0604e-05\n",
      "Gradient check: SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# =osta===========================================================================\n",
    "# PHASE 3, STEP 1 (CORRECTED): PHYSICS LOSS WITH REAL ARCHITECTURE\n",
    "# ==============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Constants and Architecture ---\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Physical Constants\n",
    "SAND_LAYER_1_L, MIDDLE_LAYER_L, TOTAL_LENGTH = 21.0, 50.0, 92.0\n",
    "POROSITY_SAND, RHO_B = 0.43, 1.5e-3\n",
    "\n",
    "# Model Hyperparameters\n",
    "NUM_KNOWN_PARAMS, LATENT_DIM, COORD_DIM, OUTPUT_DIM, LAYER_WIDTH, NUM_HIDDEN_LAYERS = 3, 64, 2, 1, 256, 4\n",
    "\n",
    "# Learnable Physical Parameter Distributions\n",
    "d_l_dist = torch.distributions.LogNormal(torch.tensor(np.log(0.01)), torch.tensor(0.5))\n",
    "alpha_dist = torch.distributions.LogNormal(torch.tensor(np.log(0.015)), torch.tensor(0.5))\n",
    "beta_dist = torch.distributions.LogNormal(torch.tensor(np.log(29.185)), torch.tensor(0.5))\n",
    "\n",
    "# --- 2. The Physics Loss Function ---\n",
    "# def compute_pde_residual(modulator, solver, coords_phys, params_phys):\n",
    "#     coords_phys.requires_grad_(True)\n",
    "    \n",
    "#     # Use the real model, not a dummy\n",
    "#     z = modulator(params_phys)\n",
    "#     c = solver(coords_phys, z)[0].mean\n",
    "    \n",
    "#     c_derivs = torch.autograd.grad(c, coords_phys, grad_outputs=torch.ones_like(c), create_graph=True)[0]\n",
    "#     c_x, c_t = c_derivs[:, 0:1], c_derivs[:, 1:2]\n",
    "    \n",
    "#     # THE FIX: Set allow_unused=True. It is possible that c_x does not depend on t,\n",
    "#     # and this was breaking the gradient graph.\n",
    "#     c_xx_grad_output = torch.autograd.grad(c_x, coords_phys, grad_outputs=torch.ones_like(c_x), create_graph=True, allow_unused=True)[0]\n",
    "#     if c_xx_grad_output is None:\n",
    "#         # Handle the case where the graph is broken for a legitimate reason (e.g., zero gradients)\n",
    "#         c_xx = torch.zeros_like(c_x)\n",
    "#     else:\n",
    "#         c_xx = c_xx_grad_output[:, 0:1]\n",
    "\n",
    "#     x_coord, porosity_middle, v_x = coords_phys[:, 0:1], params_phys[:, 0:1], params_phys[:, 1:2]\n",
    "#     D_L, alpha, beta = d_l_dist.rsample(), alpha_dist.rsample(), beta_dist.rsample()\n",
    "    \n",
    "#     interface1, interface2 = SAND_LAYER_1_L, SAND_LAYER_1_L + MIDDLE_LAYER_L\n",
    "#     is_middle_layer = (x_coord > interface1) & (x_coord < interface2)\n",
    "#     epsilon = torch.where(is_middle_layer, porosity_middle, POROSITY_SAND)\n",
    "#     alpha = torch.where(is_middle_layer, alpha, 0.0)\n",
    "#     beta = torch.where(is_middle_layer, beta, 0.0)\n",
    "    \n",
    "#     retardation_factor = 1.0 + (RHO_B / epsilon) * ((alpha * beta) / (1.0 + alpha * c)**2)\n",
    "#     pde_residual = c_t * retardation_factor - D_L * c_xx + v_x * c_x\n",
    "    \n",
    "#     return torch.mean(pde_residual**2)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# BLOCK 1: REPLACE THE `compute_pde_residual` FUNCTION\n",
    "# ==============================================================================\n",
    "def compute_pde_residual(modulator, solver, coords_phys_norm, params_phys_norm):\n",
    "    \"\"\"\n",
    "    Calculates the physics-informed residual using NORMALIZED inputs.\n",
    "    \"\"\"\n",
    "    coords_phys_norm.requires_grad_(True)\n",
    "    \n",
    "    # 1. De-normalize inputs to physical values for use inside the PDE calculation\n",
    "    coords_phys = coords_phys_norm * torch.tensor([L_MAX, T_MAX], device=device)\n",
    "    params_phys = params_phys_norm * P_RANGE + P_MIN\n",
    "\n",
    "    # 2. Forward pass uses NORMALIZED inputs\n",
    "    z = modulator(params_phys_norm)\n",
    "    c_pred_dist, kl_loss = solver(coords_phys_norm, z)\n",
    "    c = c_pred_dist.mean\n",
    "    \n",
    "    # 3. Compute gradients with respect to NORMALIZED coordinates\n",
    "    c_derivs = torch.autograd.grad(c, coords_phys_norm, grad_outputs=torch.ones_like(c), create_graph=True)[0]\n",
    "    c_x_norm, c_t_norm = c_derivs[:, 0:1], c_derivs[:, 1:2]\n",
    "    \n",
    "    # 4. Apply the Chain Rule to get derivatives in PHYSICAL dimensions\n",
    "    c_t = c_t_norm / T_MAX\n",
    "    c_x = c_x_norm / L_MAX\n",
    "    \n",
    "    c_xx_norm_grad = torch.autograd.grad(c_x_norm, coords_phys_norm, grad_outputs=torch.ones_like(c_x_norm), create_graph=True, allow_unused=True)[0]\n",
    "    \n",
    "    if c_xx_norm_grad is None:\n",
    "        c_xx = torch.zeros_like(c_x)\n",
    "    else:\n",
    "        c_xx = c_xx_norm_grad[:, 0:1] / (L_MAX**2)\n",
    "\n",
    "    # --- Assemble the PDE (using de-normalized physical values) ---\n",
    "    x_coord = coords_phys[:, 0:1]\n",
    "    porosity_middle = params_phys[:, 0:1]\n",
    "    v_x = params_phys[:, 1:2]\n",
    "    \n",
    "    D_L, alpha, beta = d_l_dist.rsample(), alpha_dist.rsample(), beta_dist.rsample()\n",
    "    \n",
    "    interface1, interface2 = SAND_LAYER_1_L, SAND_LAYER_1_L + MIDDLE_LAYER_L\n",
    "    is_middle_layer = (x_coord > interface1) & (x_coord < interface2)\n",
    "    epsilon = torch.where(is_middle_layer, porosity_middle, POROSITY_SAND)\n",
    "    alpha = torch.where(is_middle_layer, alpha, 0.0)\n",
    "    beta = torch.where(is_middle_layer, beta, 0.0)\n",
    "    \n",
    "    retardation_factor = 1.0 + (RHO_B / epsilon) * ((alpha * beta) / (1.0 + alpha * c)**2)\n",
    "    pde_residual = c_t * retardation_factor - D_L * c_xx + v_x * c_x\n",
    "    \n",
    "    return torch.mean(pde_residual**2)\n",
    "\n",
    "# # --- 3. Sanity Check Cell ---\n",
    "# if __name__ == '__main__':\n",
    "#     print(\"\\n--- Testing Physics Loss & Gradients with REAL Architecture ---\")\n",
    "    \n",
    "#     modulator = ModulatorNetwork(NUM_KNOWN_PARAMS, LATENT_DIM).to(device)\n",
    "#     solver = BayesianSolverWithFiLM(COORD_DIM, OUTPUT_DIM, LAYER_WIDTH, LATENT_DIM, NUM_HIDDEN_LAYERS).to(device)\n",
    "    \n",
    "#     dummy_coords = torch.rand(128, COORD_DIM, device=device) * torch.tensor([TOTAL_LENGTH, 500.0], device=device)\n",
    "#     dummy_params = torch.rand(128, NUM_KNOWN_PARAMS, device=device) * 0.5 + 0.5\n",
    "    \n",
    "#     loss = compute_pde_residual(modulator, solver, dummy_coords, dummy_params)\n",
    "#     print(f\"Physics loss computed: {loss.item():.4e}\")\n",
    "#     assert not torch.isnan(loss) and not torch.isinf(loss)\n",
    "\n",
    "#     print(\"\\n--- Verifying Gradient Flow ---\")\n",
    "#     params_to_check = list(modulator.parameters()) + list(solver.parameters())\n",
    "#     loss.backward()\n",
    "\n",
    "#     if all(p.grad is not None for p in params_to_check):\n",
    "#         print(\"SUCCESS: Gradients are flowing correctly to all model parameters.\")\n",
    "#     else:\n",
    "#         for name, p in modulator.named_parameters():\n",
    "#             if p.grad is None: print(f\"FAIL: Modulator param '{name}' has NO gradient.\")\n",
    "#         for name, p in solver.named_parameters():\n",
    "#             if p.grad is None: print(f\"FAIL: Solver param '{name}' has NO gradient.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# BLOCK 2: RUN THE TEST FOR THE NEW `compute_pde_residual`\n",
    "# ==============================================================================\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\n--- Testing NORMALIZED Physics Loss ---\")\n",
    "    \n",
    "    modulator = ModulatorNetwork(NUM_KNOWN_PARAMS, LATENT_DIM).to(device)\n",
    "    solver = BayesianSolverWithFiLM(COORD_DIM, OUTPUT_DIM, LAYER_WIDTH, LATENT_DIM, NUM_HIDDEN_LAYERS).to(device)\n",
    "    \n",
    "    # Create NORMALIZED dummy data in the [0, 1] range\n",
    "    dummy_coords_norm = torch.rand(128, COORD_DIM, device=device)\n",
    "    dummy_params_norm = torch.rand(128, NUM_KNOWN_PARAMS, device=device)\n",
    "    \n",
    "    modulator.zero_grad()\n",
    "    solver.zero_grad()\n",
    "\n",
    "    loss = compute_pde_residual(modulator, solver, dummy_coords_norm, dummy_params_norm)\n",
    "    (loss).backward() # Backpropagate combined loss for gradient check\n",
    "\n",
    "    print(f\"Physics loss computed: {loss.item():.4e}\")\n",
    "    grad_check = all(p.grad is not None for p in list(modulator.parameters()) + list(solver.parameters()))\n",
    "    print(f\"Gradient check: {'SUCCESS' if grad_check else 'FAIL'}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8af6226e-c211-48d3-b8ad-7e6d6af26823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing IC and BC Loss Functions and Gradients ---\n",
      "\n",
      "Verifying IC Loss and Gradients (t=0)...\n",
      "IC loss computed: 2.0421e+00\n",
      "SUCCESS: IC gradients are flowing correctly.\n",
      "\n",
      "Verifying BC Loss and Gradients (x=0)...\n",
      "BC loss computed: 6.8219e+00\n",
      "SUCCESS: BC gradients are flowing correctly.\n",
      "\n",
      "--- Verification Complete ---\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PHASE 3, STEP 2 (CORRECTED): IC/BC LOSSES WITH GRADIENT TEST\n",
    "# ==============================================================================\n",
    "\n",
    "# --- 1. Loss Function Definitions (Unchanged) ---\n",
    "\n",
    "def compute_ic_loss(modulator, solver, coords_ic, params_ic):\n",
    "    c_pred_mean = solver(coords_ic, modulator(params_ic))[0].mean\n",
    "    return torch.mean(c_pred_mean**2)\n",
    "\n",
    "def compute_bc_loss(modulator, solver, coords_bc, params_bc):\n",
    "    c_pred_mean = solver(coords_bc, modulator(params_bc))[0].mean\n",
    "    return torch.mean((c_pred_mean - 1.0)**2)\n",
    "\n",
    "# --- 2. Sanity Check Cell with Gradient Verification ---\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\n--- Testing IC and BC Loss Functions and Gradients ---\")\n",
    "    \n",
    "    # 1. Instantiate the real models\n",
    "    modulator = ModulatorNetwork(NUM_KNOWN_PARAMS, LATENT_DIM).to(device)\n",
    "    solver = BayesianSolverWithFiLM(COORD_DIM, OUTPUT_DIM, LAYER_WIDTH, LATENT_DIM, NUM_HIDDEN_LAYERS).to(device)\n",
    "    \n",
    "    # --- IC Test ---\n",
    "    print(\"\\nVerifying IC Loss and Gradients (t=0)...\")\n",
    "    batch_size_ic = 64\n",
    "    ic_coords = torch.cat([torch.rand(batch_size_ic, 1) * TOTAL_LENGTH, torch.zeros(batch_size_ic, 1)], dim=1).to(device)\n",
    "    ic_params = torch.rand(batch_size_ic, NUM_KNOWN_PARAMS, device=device)\n",
    "    \n",
    "    # Zero out any previous gradients\n",
    "    modulator.zero_grad()\n",
    "    solver.zero_grad()\n",
    "    \n",
    "    loss_ic = compute_ic_loss(modulator, solver, ic_coords, ic_params)\n",
    "    loss_ic.backward() # Backpropagate the loss\n",
    "\n",
    "    print(f\"IC loss computed: {loss_ic.item():.4e}\")\n",
    "    assert not torch.isnan(loss_ic)\n",
    "    \n",
    "    # Check for gradients\n",
    "    ic_grad_check_passed = all(p.grad is not None for p in list(modulator.parameters()) + list(solver.parameters()))\n",
    "    if ic_grad_check_passed:\n",
    "        print(\"SUCCESS: IC gradients are flowing correctly.\")\n",
    "    else:\n",
    "        print(\"FAILURE: IC gradients are missing for some parameters.\")\n",
    "\n",
    "    # --- BC Test ---\n",
    "    print(\"\\nVerifying BC Loss and Gradients (x=0)...\")\n",
    "    batch_size_bc = 64\n",
    "    bc_coords = torch.cat([torch.zeros(batch_size_bc, 1), torch.rand(batch_size_bc, 1) * 500.0], dim=1).to(device)\n",
    "    bc_params = torch.rand(batch_size_bc, NUM_KNOWN_PARAMS, device=device)\n",
    "    \n",
    "    # Zero out gradients again\n",
    "    modulator.zero_grad()\n",
    "    solver.zero_grad()\n",
    "\n",
    "    loss_bc = compute_bc_loss(modulator, solver, bc_coords, bc_params)\n",
    "    loss_bc.backward() # Backpropagate the loss\n",
    "\n",
    "    print(f\"BC loss computed: {loss_bc.item():.4e}\")\n",
    "    assert not torch.isnan(loss_bc)\n",
    "\n",
    "    # Check for gradients\n",
    "    bc_grad_check_passed = all(p.grad is not None for p in list(modulator.parameters()) + list(solver.parameters()))\n",
    "    if bc_grad_check_passed:\n",
    "        print(\"SUCCESS: BC gradients are flowing correctly.\")\n",
    "    else:\n",
    "        print(\"FAILURE: BC gradients are missing for some parameters.\")\n",
    "\n",
    "    print(\"\\n--- Verification Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "97760ea1-3b10-470d-91e8-4485eb8edeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Data Loss Function and Gradients ---\n",
      "Data loss (NLL) computed: 1.6119e+00\n",
      "Internal KL loss computed: 9.2859e+05\n",
      "\n",
      "--- Verifying Gradient Flow ---\n",
      "SUCCESS: Data loss gradients are flowing correctly.\n",
      "\n",
      "--- Verification Complete ---\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PHASE 3, STEP 3: DATA LOSS (NEGATIVE LOG-LIKELIHOOD)\n",
    "# ==============================================================================\n",
    "# Assume all previous code blocks (imports, constants, data loading, models) are in scope.\n",
    "\n",
    "# --- 1. Data Loss Function Definition ---\n",
    "def compute_data_loss(modulator, solver, data_batch):\n",
    "    \"\"\"\n",
    "    Computes the data-driven loss (Negative Log-Likelihood) on experimental data.\n",
    "    \n",
    "    Args:\n",
    "        modulator (nn.Module): The modulator network.\n",
    "        solver (nn.Module): The solver network.\n",
    "        data_batch (dict): A dictionary of tensors from our DataLoader, containing\n",
    "                           'params', 't', and 'c_over_c0'.\n",
    "    Returns:\n",
    "        Tensor: A scalar tensor representing the NLL loss.\n",
    "    \"\"\"\n",
    "    # Unpack the batch\n",
    "    params_data = data_batch['params']\n",
    "    t_data = data_batch['t']\n",
    "    c_data = data_batch['c_over_c0']\n",
    "    \n",
    "    # Create the spatial coordinate 'x'. All experimental data is at the column outlet.\n",
    "    x_coords = torch.ones_like(t_data, device=t_data.device) * TOTAL_LENGTH\n",
    "    \n",
    "    # Combine (x, t) into the coordinate tensor\n",
    "    coords_data = torch.cat([x_coords, t_data], dim=1)\n",
    "\n",
    "    # --- Forward Pass ---\n",
    "    # Get the latent code and the predicted distribution for C/C0\n",
    "    z = modulator(params_data)\n",
    "    predicted_distribution, kl_loss = solver(coords_data, z)\n",
    "    \n",
    "    # --- Loss Calculation ---\n",
    "    # The loss is the negative log-likelihood of the true data under the predicted distribution.\n",
    "    # We take the mean across the batch.\n",
    "    nll_loss = -torch.mean(predicted_distribution.log_prob(c_data))\n",
    "    \n",
    "    # We return both the NLL and the KL from the BNN layers,\n",
    "    # as both were computed during this single forward pass.\n",
    "    return nll_loss, kl_loss\n",
    "\n",
    "# --- 2. Sanity Check Cell ---\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\n--- Testing Data Loss Function and Gradients ---\")\n",
    "\n",
    "    # 1. Instantiate models\n",
    "    modulator = ModulatorNetwork(NUM_KNOWN_PARAMS, LATENT_DIM).to(device)\n",
    "    solver = BayesianSolverWithFiLM(COORD_DIM, OUTPUT_DIM, LAYER_WIDTH, LATENT_DIM, NUM_HIDDEN_LAYERS).to(device)\n",
    "    \n",
    "    # 2. Create a dummy DataLoader\n",
    "    dummy_data = {\n",
    "        'porosity': np.random.rand(100), 'v_x': np.random.rand(100) * 5 + 5,\n",
    "        'C0': np.random.rand(100) * 100 + 100, 't_min': np.random.rand(100) * 500,\n",
    "        'C_over_C0': np.random.rand(100)\n",
    "    }\n",
    "    master_df_dummy = pd.DataFrame(dummy_data).astype(np.float32)\n",
    "    dummy_dataset = BreakthroughDataset(master_df_dummy)\n",
    "    dummy_loader = DataLoader(dummy_dataset, batch_size=16)\n",
    "    \n",
    "    # 3. Get one batch and compute loss\n",
    "    first_batch = next(iter(dummy_loader))\n",
    "    # Move batch to the correct device\n",
    "    for key in first_batch:\n",
    "        first_batch[key] = first_batch[key].to(device)\n",
    "        \n",
    "    # Zero out gradients\n",
    "    modulator.zero_grad()\n",
    "    solver.zero_grad()\n",
    "    \n",
    "    data_loss, kl = compute_data_loss(modulator, solver, first_batch)\n",
    "    total_loss = data_loss + kl # For the gradient test, we combine them\n",
    "    \n",
    "    # 4. Verify loss values\n",
    "    print(f\"Data loss (NLL) computed: {data_loss.item():.4e}\")\n",
    "    print(f\"Internal KL loss computed: {kl.item():.4e}\")\n",
    "    assert not torch.isnan(data_loss) and not torch.isnan(kl)\n",
    "    \n",
    "    # 5. Gradient Test\n",
    "    print(\"\\n--- Verifying Gradient Flow ---\")\n",
    "    total_loss.backward()\n",
    "    \n",
    "    data_grad_check_passed = all(p.grad is not None for p in list(modulator.parameters()) + list(solver.parameters()))\n",
    "    if data_grad_check_passed:\n",
    "        print(\"SUCCESS: Data loss gradients are flowing correctly.\")\n",
    "    else:\n",
    "        print(\"FAILURE: Data loss gradients are missing.\")\n",
    "        \n",
    "    print(\"\\n--- Verification Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5be361d-bc64-4795-850d-0f3170d0e0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c46d64df-9232-4293-8e1c-18a6d2645eac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "--- Unified Test of All Loss Components and train_step ---\n",
      "Running multiple training iterations...\n",
      "Iteration 1\n",
      "  Total loss: 2.8968e+05\n",
      "Iteration 2\n",
      "  Total loss: 5.4394e+00\n",
      "Iteration 3\n",
      "  Total loss: 4.0111e+01\n",
      "Iteration 4\n",
      "  Total loss: 8.9989e+06\n",
      "Iteration 5\n",
      "  Total loss: 7.9592e+00\n",
      "Iteration 6\n",
      "  Total loss: 1.0615e+01\n",
      "Iteration 7\n",
      "  Total loss: 2.6060e+02\n",
      "Iteration 8\n",
      "  Total loss: 9.0806e+00\n",
      "Iteration 9\n",
      "  Total loss: 2.3000e+06\n",
      "Iteration 10\n",
      "  Total loss: 5.5874e+01\n",
      "\n",
      "--- Final Test Results ---\n",
      "Loss total: 5.5874e+01\n",
      "Loss data: 5.1589e+00\n",
      "Loss phys: 1.3582e-01\n",
      "Loss ic: 2.2340e+01\n",
      "Loss bc: 2.7595e+01\n",
      "Loss kl: 6.4548e+05\n",
      "\n",
      "SUCCESS: The full, normalized train_step is verified with multiple iterations.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# FINAL, COMPLETE AND VERIFIED TRAINING SCRIPT\n",
    "# ==============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- 1. SETUP: Constants and Device ---\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "DIAMETER, SAND_LAYER_1_L, MIDDLE_LAYER_L, TOTAL_LENGTH = 16.0, 21.0, 50.0, 92.0\n",
    "AREA_MM2 = np.pi * (DIAMETER / 2.0)**2\n",
    "POROSITY_SAND, RHO_B = 0.43, 1.5e-3\n",
    "\n",
    "NUM_KNOWN_PARAMS = 3\n",
    "LATENT_DIM = 32\n",
    "COORD_DIM = 2\n",
    "OUTPUT_DIM = 1\n",
    "LAYER_WIDTH = 128\n",
    "NUM_HIDDEN_LAYERS = 4\n",
    "EPOCHS = 1001\n",
    "BATCH_SIZE_DATA = 32\n",
    "BATCH_SIZE_BC = 64\n",
    "BATCH_SIZE_IC = 64\n",
    "BATCH_SIZE_PHYSICS = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "KL_WEIGHT = 1.0 / BATCH_SIZE_PHYSICS\n",
    "KL_ANNEALING_EPOCHS = 400\n",
    "\n",
    "\n",
    "# --- 2. DATA LOADING (Your Verified Code) ---\n",
    "master_df = create_master_dataframe(csv_files, params_list)\n",
    "master_df['v_x'] = (master_df['inflow_rate_Q'] * 1000.0) / (AREA_MM2 * master_df['porosity'])\n",
    "master_df['C_over_C0'] = master_df['C'] / master_df['C0']\n",
    "master_df['x_dummy'] = 0.0\n",
    "\n",
    "P_MIN = torch.tensor(master_df[['porosity', 'v_x', 'C0']].min().values, dtype=torch.float32, device=device)\n",
    "P_RANGE = torch.tensor(master_df[['porosity', 'v_x', 'C0']].max().values - p_min_vals, dtype=torch.float32, device=device)\n",
    "T_MAX = torch.tensor(master_df['t_min'].max(), dtype=torch.float32, device=device)\n",
    "L_MAX = torch.tensor(TOTAL_LENGTH, dtype=torch.float32, device=device)\n",
    "\n",
    "class BreakthroughDataset(Dataset):\n",
    "    # Your verified Dataset class here...\n",
    "    def __init__(self, df): self.df=df; self.p=df[['porosity','v_x','C0']].values; self.t=df[['t_min']].values; self.c=df[['C_over_C0']].values\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx): return {'params':torch.tensor(self.p[idx],dtype=torch.float32),'t':torch.tensor(self.t[idx],dtype=torch.float32),'c_over_c0':torch.tensor(self.c[idx],dtype=torch.float32)}\n",
    "\n",
    "data_dataset = BreakthroughDataset(master_df)\n",
    "data_loader = DataLoader(data_dataset, batch_size=BATCH_SIZE_DATA, shuffle=True)\n",
    "\n",
    "# --- 3. ARCHITECTURE ---\n",
    "\n",
    "\n",
    "class DenseBayesian(nn.Module):\n",
    "    def __init__(self, in_features, out_features, activation=None):\n",
    "        super().__init__()\n",
    "        self.in_features, self.out_features = in_features, out_features\n",
    "        self.kernel_mu = nn.Parameter(torch.empty(in_features, out_features))\n",
    "        self.kernel_rho = nn.Parameter(torch.empty(in_features, out_features))\n",
    "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "        self.bias_rho = nn.Parameter(torch.empty(out_features))\n",
    "        nn.init.xavier_uniform_(self.kernel_mu)\n",
    "        nn.init.normal_(self.kernel_rho, mean=-3., std=1.)\n",
    "        nn.init.zeros_(self.bias_mu)\n",
    "        nn.init.normal_(self.bias_rho, mean=-3., std=1.)\n",
    "        self.prior = torch.distributions.Laplace(0, 1.)\n",
    "        self.activation = activation() if activation else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        kernel_sigma = torch.nn.functional.softplus(self.kernel_rho)\n",
    "        bias_sigma = torch.nn.functional.softplus(self.bias_rho)\n",
    "        kernel_posterior = torch.distributions.Normal(self.kernel_mu, kernel_sigma)\n",
    "        bias_posterior = torch.distributions.Normal(self.bias_mu, bias_sigma)\n",
    "        w, b = kernel_posterior.rsample(), bias_posterior.rsample()\n",
    "        kl_divergence = (kernel_posterior.log_prob(w) - self.prior.log_prob(w)).sum() + \\\n",
    "                        (bias_posterior.log_prob(b) - self.prior.log_prob(b)).sum()\n",
    "        return self.activation(torch.matmul(x, w) + b), kl_divergence\n",
    "        \n",
    "\n",
    "class ModulatorNetwork(nn.Module):\n",
    "    \"\"\"Encodes known experimental parameters [ε, v_x, C₀] into a latent vector 'z'.\"\"\"\n",
    "    def __init__(self, input_param_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        # self.net = nn.Sequential(\n",
    "        #     nn.Linear(input_param_dim, 64),\n",
    "        #     nn.Tanh(),\n",
    "        #     nn.Linear(64, 64),\n",
    "        #     nn.Tanh(),\n",
    "        #     nn.Linear(64, latent_dim)\n",
    "        # )\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_param_dim, latent_dim),\n",
    "            nn.Linear(latent_dim, latent_dim)\n",
    "        )\n",
    "    def forward(self, physical_params):\n",
    "        return self.net(physical_params)\n",
    "        \n",
    "\n",
    "class FiLMLayer(nn.Module):\n",
    "    \"\"\"Feature-wise Linear Modulation Layer.\"\"\"\n",
    "    def __init__(self, num_features, latent_dim):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.projection = nn.Linear(latent_dim, num_features * 2)\n",
    "\n",
    "    def forward(self,x,z):\n",
    "        gamma,beta=self.projection(z).chunk(2,dim=-1);\n",
    "        return gamma*x+beta\n",
    "\n",
    "class BayesianSolverWithFiLM(nn.Module):\n",
    "    \"\"\"A Bayesian Solver that uses FiLM layers for conditioning.\"\"\"\n",
    "    def __init__(self, coord_dim, output_dim, layer_width, latent_dim, num_hidden_layers):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.input_dense = DenseBayesian(coord_dim, layer_width, activation=nn.Tanh)\n",
    "\n",
    "\n",
    "        self.hidden_blocks = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                'dense': DenseBayesian(layer_width, layer_width),\n",
    "                'film': FiLMLayer(layer_width, latent_dim),\n",
    "                'activation': nn.Tanh()\n",
    "            }) for _ in range(num_hidden_layers)\n",
    "        ])\n",
    "\n",
    "        self.output_dense = DenseBayesian(layer_width, output_dim * 2)\n",
    "\n",
    "    def forward(self, coords, z):\n",
    "        total_kl_loss = 0.\n",
    "        \n",
    "        x, kl = self.input_dense(coords)\n",
    "        total_kl_loss += kl\n",
    "        \n",
    "        for block in self.hidden_blocks:\n",
    "            x_res, kl = block['dense'](x)\n",
    "            total_kl_loss += kl\n",
    "            \n",
    "            x_mod = block['film'](x_res, z)\n",
    "            # Add a residual connection (as in your TF code)\n",
    "            x = block['activation'](x_mod) + x_res\n",
    "\n",
    "        raw_output, kl = self.output_dense(x)\n",
    "        total_kl_loss += kl\n",
    "        \n",
    "        mu = raw_output[..., :self.output_dim]\n",
    "        sigma_raw = raw_output[..., self.output_dim:]\n",
    "        sigma = torch.nn.functional.softplus(sigma_raw) + 1e-6\n",
    "        \n",
    "        return torch.distributions.Normal(loc=mu, scale=sigma), total_kl_loss\n",
    "\n",
    "# --- 4. Loss Functions ---\n",
    "\n",
    "def compute_pde_residual(modulator, solver, coords_norm, params_norm):\n",
    "    # Create a fresh copy and set requires_grad\n",
    "    coords_norm = coords_norm.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    c, kl_phys = solver(coords_norm, modulator(params_norm))\n",
    "    c_mean = c.mean\n",
    "    \n",
    "    c_derivs = torch.autograd.grad(c_mean, coords_norm, torch.ones_like(c_mean), create_graph=True)[0]\n",
    "    c_x_norm, c_t_norm = c_derivs[:, 0:1], c_derivs[:, 1:2]\n",
    "    \n",
    "    # Use create_graph=True but not retain_graph\n",
    "    c_xx_norm_grad = torch.autograd.grad(c_x_norm, coords_norm, torch.ones_like(c_x_norm), create_graph=True, allow_unused=True)[0]\n",
    "    \n",
    "    # De-normalize everything for the physical calculation\n",
    "    coords = coords_norm * torch.tensor([L_MAX, T_MAX], device=device)\n",
    "    params = params_norm * P_RANGE + P_MIN\n",
    "    c_t = c_t_norm / T_MAX\n",
    "    c_x = c_x_norm / L_MAX\n",
    "    c_xx = c_xx_norm_grad[:, 0:1] / (L_MAX**2) if c_xx_norm_grad is not None else torch.zeros_like(c_x)\n",
    "\n",
    "    x_coord, porosity_middle, v_x = coords[:, 0:1], params[:, 0:1], params[:, 1:2]\n",
    "    \n",
    "    # Sample from distributions without modifying them\n",
    "    D_L = d_l_dist.rsample()\n",
    "    alpha = alpha_dist.rsample()\n",
    "    beta = beta_dist.rsample()\n",
    "    \n",
    "    i1, i2 = SAND_LAYER_1_L, SAND_LAYER_1_L + MIDDLE_LAYER_L\n",
    "    is_mid = (x_coord > i1) & (x_coord < i2)\n",
    "    \n",
    "    # Use where instead of in-place operations\n",
    "    eps = torch.where(is_mid, porosity_middle, torch.tensor(POROSITY_SAND, device=device))\n",
    "    alpha_val = torch.where(is_mid, alpha, torch.tensor(0., device=device))\n",
    "    beta_val = torch.where(is_mid, beta, torch.tensor(0., device=device))\n",
    "    \n",
    "    R = 1.0 + (RHO_B / eps) * ((alpha_val * beta_val) / (1.0 + alpha_val * c_mean)**2)\n",
    "    pde_residual = c_t * R - D_L * c_xx + v_x * c_x\n",
    "    \n",
    "    return torch.mean(pde_residual**2), kl_phys\n",
    "\n",
    "def compute_data_loss(modulator, solver, coords_norm, params_norm, c_data):\n",
    "    pred_dist, kl_loss = solver(coords_norm, modulator(params_norm))\n",
    "    nll = -torch.mean(pred_dist.log_prob(c_data))\n",
    "    return nll, kl_loss\n",
    "\n",
    "def compute_ic_loss(modulator, solver, coords_norm, params_norm):\n",
    "    c_pred, kl_ic = solver(coords_norm, modulator(params_norm))\n",
    "    c_pred = c_pred.mean\n",
    "    return torch.mean(c_pred**2), kl_ic\n",
    "\n",
    "def compute_bc_loss(modulator, solver, coords_norm, params_norm):\n",
    "    c_pred, kl_bc = solver(coords_norm, modulator(params_norm))\n",
    "    c_pred = c_pred.mean\n",
    "    return torch.mean((c_pred - 1.0)**2), kl_bc\n",
    "\n",
    "# --- 5. Training Step and Loop ---\n",
    "\n",
    "def train_step(modulator, solver, optimizer, batch, current_kl_weight):\n",
    "    # Clear gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # --- Loss Calculation ---\n",
    "    # Create fresh copies of tensors\n",
    "    coords_data_norm = batch['coords_data_norm'].clone().detach()\n",
    "    params_data_norm = batch['params_data_norm'].clone().detach()\n",
    "    c_data = batch['c_data'].clone().detach()\n",
    "    \n",
    "    coords_phys_norm = batch['coords_phys_norm'].clone().detach()\n",
    "    params_phys_norm = batch['params_phys_norm'].clone().detach()\n",
    "    \n",
    "    coords_ic_norm = batch['coords_ic_norm'].clone().detach()\n",
    "    params_ic_norm = batch['params_ic_norm'].clone().detach()\n",
    "    \n",
    "    coords_bc_norm = batch['coords_bc_norm'].clone().detach()\n",
    "    params_bc_norm = batch['params_bc_norm'].clone().detach()\n",
    "    \n",
    "    # Calculate losses with cloned tensors\n",
    "    loss_data, kl_from_data_pass = compute_data_loss(modulator, solver, coords_data_norm, params_data_norm, c_data)\n",
    "    loss_phys, kl_phys = compute_pde_residual(modulator, solver, coords_phys_norm, params_phys_norm)\n",
    "    loss_ic, kl_ic = compute_ic_loss(modulator, solver, coords_ic_norm, params_ic_norm)\n",
    "    loss_bc, kl_bc = compute_bc_loss(modulator, solver, coords_bc_norm, params_bc_norm)\n",
    "\n",
    "    # Aggregate KL from all forward passes\n",
    "    kl_weights = kl_from_data_pass + kl_phys + kl_ic + kl_bc\n",
    "    \n",
    "    # KL for our learnable physical parameters\n",
    "    kl_params = (torch.distributions.kl_divergence(d_l_dist, torch.distributions.LogNormal(0., 1.)) +\n",
    "                 torch.distributions.kl_divergence(alpha_dist, torch.distributions.LogNormal(0., 1.)) +\n",
    "                 torch.distributions.kl_divergence(beta_dist, torch.distributions.LogNormal(0., 1.)))\n",
    "\n",
    "    \n",
    "    # --- Total Loss ---\n",
    "    total_loss = loss_data + loss_phys + loss_ic + loss_bc + current_kl_weight * (kl_weights + kl_params)\n",
    "\n",
    "    # --- Backpropagation ---\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Create a new dictionary with detached tensors\n",
    "    loss_dict = {\n",
    "        \"total\": total_loss.detach().clone(),\n",
    "        \"data\": loss_data.detach().clone(), \n",
    "        \"phys\": loss_phys.detach().clone(), \n",
    "        \"ic\": loss_ic.detach().clone(), \n",
    "        \"bc\": loss_bc.detach().clone(), \n",
    "        \"kl\": (kl_weights + kl_params).detach().clone()\n",
    "    }\n",
    "    \n",
    "    return loss_dict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    log_D_L_loc = torch.nn.Parameter(torch.log(torch.tensor(0.01)))\n",
    "    log_D_L_scale = torch.nn.Parameter(torch.tensor(0.5))\n",
    "    log_alpha_loc = torch.nn.Parameter(torch.log(torch.tensor(0.015)))\n",
    "    log_alpha_scale = torch.nn.Parameter(torch.tensor(0.5))\n",
    "    log_beta_loc = torch.nn.Parameter(torch.log(torch.tensor(29.185)))\n",
    "    log_beta_scale = torch.nn.Parameter(torch.tensor(0.5))\n",
    "    \n",
    "    # d_l_dist = torch.distributions.LogNormal(log_D_L_loc, torch.nn.functional.softplus(log_D_L_scale))\n",
    "    # alpha_dist = torch.distributions.LogNormal(log_alpha_loc, torch.nn.functional.softplus(log_alpha_scale))\n",
    "    # beta_dist = torch.distributions.LogNormal(log_beta_loc, torch.nn.functional.softplus(log_beta_scale))\n",
    "\n",
    "    print(\"\\n--- Unified Test of All Loss Components and train_step ---\")\n",
    "\n",
    "    modulator = ModulatorNetwork(NUM_KNOWN_PARAMS, LATENT_DIM).to(device)\n",
    "    solver = BayesianSolverWithFiLM(COORD_DIM, OUTPUT_DIM, LAYER_WIDTH, LATENT_DIM, NUM_HIDDEN_LAYERS).to(device)\n",
    "    \n",
    "    # The list of all trainable parameters must be passed to the optimizer\n",
    "    all_trainable_vars = list(modulator.parameters()) + list(solver.parameters()) + \\\n",
    "                         [log_D_L_loc, log_D_L_scale, log_alpha_loc, log_alpha_scale, log_beta_loc, log_beta_scale]\n",
    "    optimizer = torch.optim.Adam(all_trainable_vars, lr=LEARNING_RATE)\n",
    "\n",
    "    # --- Initialize Sobol Samplers ---\n",
    "    sobol_engine_phys = torch.quasirandom.SobolEngine(dimension=2)\n",
    "    sobol_engine_params = torch.quasirandom.SobolEngine(dimension=3)\n",
    "\n",
    "    \n",
    "    # # --- Prepare a single, comprehensive batch dictionary ---\n",
    "    # batch = {}\n",
    "\n",
    "    # # 1. Data points (using standard random sampling from dataframe)\n",
    "    # data_sample = master_df.sample(n=BATCH_SIZE_DATA)\n",
    "    # params_data = torch.tensor(data_sample[['porosity', 'v_x', 'C0']].values, device=device)\n",
    "    # t_data = torch.tensor(data_sample[['t_min']].values, device=device)\n",
    "    # c_data = torch.tensor(data_sample[['C_over_C0']].values, device=device)\n",
    "    # x_data = torch.ones_like(t_data) * TOTAL_LENGTH\n",
    "    \n",
    "    # # Normalize and add to batch\n",
    "    # batch['c_data'] = c_data\n",
    "    # batch['coords_data_norm'] = (torch.cat([x_data, t_data], dim=1) / torch.tensor([L_MAX, T_MAX], device=device))\n",
    "    # batch['params_data_norm'] = ((params_data - P_MIN) / P_RANGE)\n",
    "\n",
    "    # # 2. Physics points (using Sobol sampling)\n",
    "    # phys_params_norm = sobol_engine_params.draw(BATCH_SIZE_PHYSICS).to(device)\n",
    "    # phys_coords_norm = sobol_engine_phys.draw(BATCH_SIZE_PHYSICS).to(device)\n",
    "    # batch['params_phys_norm'] = phys_params_norm\n",
    "    # batch['coords_phys_norm'] = phys_coords_norm\n",
    "    \n",
    "    # # 3. IC points (using Sobol sampling)\n",
    "    # ic_params_norm = sobol_engine_params.draw(BATCH_SIZE_IC).to(device)\n",
    "    # # Sobol for x, combined with zeros for t\n",
    "    # ic_coords_norm = torch.cat([sobol_engine_phys.draw(BATCH_SIZE_IC)[:, 0:1].to(device), torch.zeros(BATCH_SIZE_IC, 1, device=device)], dim=1)\n",
    "    # batch['params_ic_norm'] = ic_params_norm\n",
    "    # batch['coords_ic_norm'] = ic_coords_norm\n",
    "\n",
    "    # # 4. BC points (using Sobol sampling)\n",
    "    # bc_params_norm = sobol_engine_params.draw(BATCH_SIZE_BC).to(device)\n",
    "    # # Zeros for x, Sobol for t\n",
    "    # bc_coords_norm = torch.cat([torch.zeros(BATCH_SIZE_BC, 1, device=device), sobol_engine_phys.draw(BATCH_SIZE_BC)[:, 1:2].to(device)], dim=1)\n",
    "    # batch['params_bc_norm'] = bc_params_norm\n",
    "    # batch['coords_bc_norm'] = bc_coords_norm\n",
    "    \n",
    "    # print(\"Batch prepared successfully using Sobol sampling.\")\n",
    "    \n",
    "    # # --- Execute and Test ---\n",
    "    # # The train_step function itself is correct now that the decorator is removed.\n",
    "    # loss_dict = train_step(modulator, solver, optimizer, batch, current_kl_weight=0.1)\n",
    "\n",
    "    # print(\"\\n--- Test Results ---\")\n",
    "    # for key, val in loss_dict.items():\n",
    "    #     print(f\"Loss {key}: {val.item():.4e}\")\n",
    "    #     assert not torch.isnan(val)\n",
    "    # print(\"\\nSUCCESS: The full, normalized train_step is verified with Sobol sampling.\")\n",
    "\n",
    "\n",
    "        # --- Execute and Test ---\n",
    "    # Run multiple iterations safely\n",
    "    # --- Execute and Test ---\n",
    "    print(\"Running multiple training iterations...\")\n",
    "    \n",
    "    for i in range(10):\n",
    "        print(f\"Iteration {i+1}\")\n",
    "        \n",
    "        # Recreate the distributions for each iteration\n",
    "        d_l_dist = torch.distributions.LogNormal(log_D_L_loc, torch.nn.functional.softplus(log_D_L_scale))\n",
    "        alpha_dist = torch.distributions.LogNormal(log_alpha_loc, torch.nn.functional.softplus(log_alpha_scale))\n",
    "        beta_dist = torch.distributions.LogNormal(log_beta_loc, torch.nn.functional.softplus(log_beta_scale))\n",
    "        \n",
    "        # Create a fresh batch for each iteration\n",
    "        fresh_batch = {}\n",
    "        \n",
    "        # 1. Data points\n",
    "        data_sample = master_df.sample(n=BATCH_SIZE_DATA)\n",
    "        params_data = torch.tensor(data_sample[['porosity', 'v_x', 'C0']].values, device=device)\n",
    "        t_data = torch.tensor(data_sample[['t_min']].values, device=device)\n",
    "        c_data = torch.tensor(data_sample[['C_over_C0']].values, device=device)\n",
    "        x_data = torch.ones_like(t_data) * TOTAL_LENGTH\n",
    "        \n",
    "        fresh_batch['c_data'] = c_data\n",
    "        fresh_batch['coords_data_norm'] = (torch.cat([x_data, t_data], dim=1) / torch.tensor([L_MAX, T_MAX], device=device))\n",
    "        fresh_batch['params_data_norm'] = ((params_data - P_MIN) / P_RANGE)\n",
    "    \n",
    "        # 2. Physics points\n",
    "        fresh_batch['params_phys_norm'] = sobol_engine_params.draw(BATCH_SIZE_PHYSICS).to(device)\n",
    "        fresh_batch['coords_phys_norm'] = sobol_engine_phys.draw(BATCH_SIZE_PHYSICS).to(device)\n",
    "        \n",
    "        # 3. IC points\n",
    "        fresh_batch['params_ic_norm'] = sobol_engine_params.draw(BATCH_SIZE_IC).to(device)\n",
    "        fresh_batch['coords_ic_norm'] = torch.cat([\n",
    "            sobol_engine_phys.draw(BATCH_SIZE_IC)[:, 0:1].to(device), \n",
    "            torch.zeros(BATCH_SIZE_IC, 1, device=device)\n",
    "        ], dim=1)\n",
    "    \n",
    "        # 4. BC points\n",
    "        fresh_batch['params_bc_norm'] = sobol_engine_params.draw(BATCH_SIZE_BC).to(device)\n",
    "        fresh_batch['coords_bc_norm'] = torch.cat([\n",
    "            torch.zeros(BATCH_SIZE_BC, 1, device=device), \n",
    "            sobol_engine_phys.draw(BATCH_SIZE_BC)[:, 1:2].to(device)\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Run a single training step with the fresh batch\n",
    "        loss_dict = train_step(modulator, solver, optimizer, fresh_batch, current_kl_weight=1e-6)\n",
    "        print(f\"  Total loss: {loss_dict['total'].item():.4e}\")\n",
    "        \n",
    "        # Clear any cached tensors\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"\\n--- Final Test Results ---\")\n",
    "    for key, val in loss_dict.items():\n",
    "        print(f\"Loss {key}: {val.item():.4e}\")\n",
    "        assert not torch.isnan(val)\n",
    "    print(\"\\nSUCCESS: The full, normalized train_step is verified with multiple iterations.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543eb660-61cc-4623-8795-a5d9a9edbf78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f9fb94-4319-47f0-9dfe-d01116f87998",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda-ml-ai]",
   "language": "python",
   "name": "conda-env-anaconda-ml-ai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
